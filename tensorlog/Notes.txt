 - export PYTHONPATH=/usr/local/google/home/cohenw/code/TensorLog:$PYTHONPATH
 - live theano add /usr/local/google/home/cohenw/code/Theano

Next actions (KR):

 - revive debug.py - how can testing this be automated? should I bother? w/ Katie?
 - look at sparse messages in theano-based learner?

Next actions (WC):
 - simple: 
 -- serialization doesn't work in provenance/synthetic/simple-expt?
 -- regularization doesn't work well with reparameterization
 -- provenance/synthetic/simple-expt.py -- test
 -- l1-regularization on synthetic/simple-expt.py and real-data expt

Little jobs:
 - tests and etc for {id} features.  Should I have these? they are just {weighted(R1): assign(R1,r1,ruleid_t)}
 - cleanup sessions in tensorflowxcomp
 - cleanup modes? should they always be written pred/io, and i,o no longer be reserved words?
 - robust parsing of programs - figure out if it's proppr format or not? make rules weights optional?
 - interp.helpConfig()
 - output types as typename/id
 - parsing is not robust --- eg, failures on 'musicby(X,Y) :- head(Trip,X),tail(Trip,Y),{weighted(C): creator(Trip,C)}.'
 - move testability code to subclass "testabletensorflowxcompiler"

Semantics:
 - tensorlog compiler compiles p_io(x) to v=softmax(proofcount(y|x,db,prog,theta))
 - alternative: make proofcount(y|x) the a core output, and configure inference schemes
   on top of this -- either in compiler or just in tensorflow

Bigger steps:
 - automatically move program constants into the database and introduce assign clauses when needed?? -nah
 - benchmark tests on tensorflow
 - adding tensorflow 'builtins'
 -- bpcompiler: 
 --- code is mostly ready, based on BuiltInIOOp, just need to add a "virtual op" type.
 --- need to worry about checking modes, and multiple inputs
 -- xcomp virtualDB: maps modes to 
 --- p(i,o) -> lambda inputExpression:outputExpression 
 --- p(o)   -> lambda outputExpression 
 - snake_case fixes??
 - repackage (see below)???
 - optimize compilation???
 - typing for tensorlog -done
 -- note: 10k sample has 3,434 people, 11,819 entities, larger yago has 59,120 people 118,703 entities
    so it's 10x bigger (in message size); 10k sample uses about 15 Gb memory
 -- should look at if sparse matrices are actually being used and monitor where the space is going

 -- declare.typeDeclaration -done
 -- cfacts parser allows #declare foo type1 type2 (insist everything typed or nothing, based on config?)
    (see bufferFile/bufferLine in matrixDB -done
 -- matrixDB keeps a symtab for each type -done
 -- bpcompiler infers type of every variable -done
 -- funs maintains/returns type -done
 -- docs -done
 -- check type inference for sums -done

Overall package structure:
  tensorlog: config, matrixdb, parser, bpcomp(iler), program, funs, ops (-eval and bprop), xcomp
    .ui: comline, expt, list, debug  
    .native: mutil, autodiff (eval and bprop), learn, plearn, putil, dataset
    .th: theanoxcomp
    .tf: tensorflowxcomp
  or maybe just stick a bunch of stuff in tlg.native: native.learn, ...

Proposed simple -  _ means python object or else argument of comline.parseFOO
  - tlg.Compiler(
        compilerClass=_ | trget='tensorflow',
    	program= _    
      	type_declarations= _
      	db= _,
      	regularizer= ...
      	summaryFile=...)

Next actions:

   - upload dialog-toy

Question->query idea

 for each property pi(X,t) where t is a tag and X the set of things
 which have that property, use the rules

   q1(Q,X) :- pi_query_tag(Q,T), pi(X,T), {pi_relevant(F): query_feature(Q,F)}
   q2(Q,X) :- anything(X) {pi_irrelevant(F): query_feature(Q,F)}
   ...
   qn(Q,X) :- anything(X) {pn_irrelevant(F): query_feature(Q,F)}
   
   q(Q,X) :- q1(Q,X),q2(Q,X),... qn(Q,X)

 pi_query_tag(Q,T) : tag T for property pi is in query, eg "T=red" for pi=color in "a red sweater vest"
 query_feature(Q,F) : words/ngrams etc in query
 
--------------------

movie app idea: 
 - train inference using provenance features

triple Trip has: head(Trip,H),tail(Trip,H),rel(Trip,R),creator(Trip,C)

 | head	rxy	x
 | tail	rxy	y
 | creator	rxy	nyt
 | creator	rxy	fox
 | rel	rxy	r


for predicate p(Slot,Filler):-r(Slot,Filler) inference rule is:

 | p(Slot,Filler) :- 
 |   head(Trip,Slot),assign(R,r),rel(Trip,R),tail(Trip,Filler) 
 |   creator(Trip,C), weighted(C).

for predicate p(Slot,Filler):-r1(Slot,Z),r2(Z,Filler) inference rule is:

 | p(Slot,Filler) :- 
 |     head(Trip1,Slot),assign(R1,r1), rel(Trip1,R1), tail(Trip1,Z)
 |     head(Trip2,Z),   assign(R2,r2), rel(Trip2,R2), tail(Trip2,Filler)
 |     creator(Trip1,C1), weighted(C1), creator(Trip2,C2), weighted(C2).
 
Then train high-confidence results against low-confidence ones.

 - might be better to include relation name 'rel' in the
  head/tail/creator triple, eg r1_head(Trip,H), r1_tail(Trip,H),
  r1_creator(Trip,C)

 - if I get multi-mode training working then you could do a bit more,
 eg train against several preds at once, or include ssl-like
 constraints... except, will they work in Tensorlog? not sure...but
 you could introduce an explicit entropy penalty for answer to
 p_conflict

 p_conflict(Slot,Filler) :- p(Slot,Filler)
