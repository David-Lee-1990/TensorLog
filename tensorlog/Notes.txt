 - export PYTHONPATH=/usr/local/google/home/cohenw/code/TensorLog:$PYTHONPATH
 - live theano add /usr/local/google/home/cohenw/code/Theano

Katie actions:

 - fix pyparsing
 - revive debug.py - how can testing this be automated? should I bother? w/ Katie?
 - look at sparse messages in theano-based learner?
 - look at expt() in theano-based learner

Status:
 - can now handle large version of dialog-toy, which is 270k facts, typed
 - can I do it untyped?

Bigger steps:

 - multi-mode training and compilation -- needed for movie app
 - automatically move program constants into the database and introduce assign clauses when needed??
 - benchmark tests on tensorflow
 - why can't I include a product/join? eg, add a built-in and(X1,...,Xn,Y) with
   mode equal(i,i,...,i,o) which outputs Y as the component-wise product of all the X's?
   then you could write q(X,Y) :- q1(X,Y1),....,qk(X,Yk),and(Y1,...,Yk,Y)
 - adding tensorflow 'builtins'
 -- p(i,i,o) -> lambda inputExpression1,inputExpression2:outputExpression 
 -- p(i,o) -> lambda inputExpression:outputExpression 
 -- p(o)   -> lambda outputExpression 
 -- do you need to do planning to find a valid path? re-ordering?
 - snake_case fixes??
 - repackage (see below)???
 - optimize compilation???

Overall package structure:
  tensorlog: config, matrixdb, parser, bpcomp(iler), program, funs, ops (-eval and bprop), xcomp
    .ui: comline, expt, list, debug  
    .native: mutil, autodiff (eval and bprop), learn, plearn, putil, dataset
    .th: theanoxcomp
    .tf: tensorflowxcomp

Question->query idea

 for each property pi(X,t) where t is a tag and X the set of things
 which have that property, use the rules

   q1(Q,X) :- pi_query_tag(Q,T), pi(X,T), {pi_relevant(F): query_feature(Q,F)}
   q2(Q,X) :- anything(X) {pi_irrelevant(F): query_feature(Q,F)}
   ...
   qn(Q,X) :- anything(X) {pn_irrelevant(F): query_feature(Q,F)}
   
   q(Q,X) :- q1(Q,X),q2(Q,X),... qn(Q,X)

 pi_query_tag(Q,T) : tag T for property pi is in query, eg "T=red" for pi=color in "a red sweater vest"
 query_feature(Q,F) : words/ngrams etc in query
 
--------------------

movie app idea: 
 - train inference using provenance features

triple Trip has: head(Trip,H),tail(Trip,H),rel(Trip,R),creator(Trip,C)

 | head	rxy	x
 | tail	rxy	y
 | creator	rxy	nyt
 | creator	rxy	fox
 | rel	rxy	r


for predicate p(Slot,Filler):-r(Slot,Filler) inference rule is:

 | p(Slot,Filler) :- 
 |   head(Trip,Slot),assign(R,r),rel(Trip,R),tail(Trip,Filler) 
 |   creator(Trip,C), weighted(C).

for predicate p(Slot,Filler):-r1(Slot,Z),r2(Z,Filler) inference rule is:

 | p(Slot,Filler) :- 
 |     head(Trip1,Slot),assign(R1,r1), rel(Trip1,R1), tail(Trip1,Z)
 |     head(Trip2,Z),   assign(R2,r2), rel(Trip2,R2), tail(Trip2,Filler)
 |     creator(Trip1,C1), weighted(C1), creator(Trip2,C2), weighted(C2).
 
Then train high-confidence results against low-confidence ones.

 - might be better to include relation name 'rel' in the
  head/tail/creator triple, eg r1_head(Trip,H), r1_tail(Trip,H),
  r1_creator(Trip,C)

 - if I get multi-mode training working then you could do a bit more,
 eg train against several preds at once, or include ssl-like
 constraints... except, will they work in Tensorlog? not sure...but
 you could introduce an explicit entropy penalty for answer to
 p_conflict

 p_conflict(Slot,Filler) :- p(Slot,Filler)
