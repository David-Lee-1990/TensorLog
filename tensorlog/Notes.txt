 - export PYTHONPATH=/usr/local/google/home/cohenw/code/TensorLog:$PYTHONPATH
 - live theano add /usr/local/google/home/cohenw/code/Theano

Next actions:

 - revive debug.py - how can testing this be automated? should I bother? w/ Katie?
 - fix pyparsing
 - look at sparse messages in theano-based learner?

Bigger steps:

 - automatically move program constants into the database and introduce assign clauses when needed??
 - multi-mode training and compilation -- needed for movie app
 - benchmark tests on tensorflow
 - adding tensorflow 'builtins'
 -- p(i,o) -> lambda inputExpression:outputExpression 
 -- p(o)   -> lambda outputExpression 
 - snake_case fixes??
 - repackage (see below)???
 - optimize compilation???
 - typing for tensorlog 
 -- note: 10k sample has 3,434 people, 11,819 entities, larger yago has 59,120 people 118,703 entities
    so it's 10x bigger (in message size); 10k sample uses about 15 Gb memory
 -- should look at if sparse matrices are actually being used and monitor where the space is going

 -- declare.typeDeclaration -done
 -- cfacts parser allows #declare foo type1 type2 (insist everything typed or nothing, based on config?)
    (see bufferFile/bufferLine in matrixDB -done
 -- matrixDB keeps a symtab for each type -done
 -- bpcompiler infers type of every variable -done
 -- funs maintains/returns type -done
 -- docs -done
 -- check type inference for sums -done

Overall package structure:
  tensorlog: config, matrixdb, parser, bpcomp(iler), program, funs, ops (-eval and bprop), xcomp
    .ui: comline, expt, list, debug  
    .native: mutil, autodiff (eval and bprop), learn, plearn, putil, dataset
    .th: theanoxcomp
    .tf: tensorflowxcomp

Question->query idea

 for each property pi(X,t) where t is a tag and X the set of things
 which have that property, use the rules

   q1(Q,X) :- pi_query_tag(Q,T), pi(X,T), {pi_relevant(F): query_feature(Q,F)}
   q2(Q,X) :- anything(X) {pi_irrelevant(F): query_feature(Q,F)}
   ...
   qn(Q,X) :- anything(X) {pn_irrelevant(F): query_feature(Q,F)}
   
   q(Q,X) :- q1(Q,X),q2(Q,X),... qn(Q,X)

 pi_query_tag(Q,T) : tag T for property pi is in query, eg "T=red" for pi=color in "a red sweater vest"
 query_feature(Q,F) : words/ngrams etc in query
 
--------------------

movie app idea: 
 - train inference using provenance features

triple Trip has: head(Trip,H),tail(Trip,H),rel(Trip,R),creator(Trip,C)

 | head	rxy	x
 | tail	rxy	y
 | creator	rxy	nyt
 | creator	rxy	fox
 | rel	rxy	r


for predicate p(Slot,Filler):-r(Slot,Filler) inference rule is:

 | p(Slot,Filler) :- 
 |   head(Trip,Slot),assign(R,r),rel(Trip,R),tail(Trip,Filler) 
 |   creator(Trip,C), weighted(C).

for predicate p(Slot,Filler):-r1(Slot,Z),r2(Z,Filler) inference rule is:

 | p(Slot,Filler) :- 
 |     head(Trip1,Slot),assign(R1,r1), rel(Trip1,R1), tail(Trip1,Z)
 |     head(Trip2,Z),   assign(R2,r2), rel(Trip2,R2), tail(Trip2,Filler)
 |     creator(Trip1,C1), weighted(C1), creator(Trip2,C2), weighted(C2).
 
Then train high-confidence results against low-confidence ones.

 - might be better to include relation name 'rel' in the
  head/tail/creator triple, eg r1_head(Trip,H), r1_tail(Trip,H),
  r1_creator(Trip,C)

 - if I get multi-mode training working then you could do a bit more,
 eg train against several preds at once, or include ssl-like
 constraints... except, will they work in Tensorlog? not sure...but
 you could introduce an explicit entropy penalty for answer to
 p_conflict

 p_conflict(Slot,Filler) :- p(Slot,Filler)
