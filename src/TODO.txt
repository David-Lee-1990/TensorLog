python tensorlog.py --proppr --debug --programFiles test/textcat.ppr:test/textcattoy.cfacts 'predict(rw,Y)' 

dlogP[r,y]/dW[i,j] = 1/P[r,y] * dP[r,y]/dW[i,j]
dlogP[r,y]/dW[R,j] = 1/P[r,y] * dP[r,y]/dW[r,j]  since partial is 0 for r!=i

this involves tensors - maybe the best approach is to write a scalar
CrossEntropy function with a custom, non-matrix valued grad() method.

To think through: forward vs reverse-mode differentiation

Theano-free learning:
- add normalization operator -done
- revive matrix computation -done
- stripped out multiple function outputs - done
- make bpcompiler.py return a Fun -done
- make tensorlog.py manipulate Funs, not Ops -done

learn.py:
 - dataset loader (see testtensorlog.py)
 - loss function in funs? in learn?
   loss = B.sp_sum(-y * B.structured_log(prob),sparse_grad=True)   #cross-entropy loss

   0 <= Y[i,y] <= 1  hard labels
   0 <= P[i,y] <= 1  predictions
   
   loss = sum_{i,y nonz in P} { -Y[i,y]*log(P[i,y]) }

   chain rule: dloss/dw = { -Y[i,y] * 1/P[i,y] * dP[i,y]/dw }

   generally when I have a matrix function like P(w) there will be
   dP[i,y]/dw for every P[i,y] and every w.

   reverse mode: focusing deeper computations by at each level, have a
   matrix of [i,j] pairs of interest, and always return a new set of
   pairs of interest which have been extended by using the chain rule.

   level by level:
   - top: -Y*fun focus is 1, pass Y to dfun/dw
   - next: logP focus is Y, pass Y*1/P to dP/dw
   - next  norm(P) focus is Y*(1/P) pass Y*(1/P)*(1/g^2)(gf' - fg') to f'=dP/dw, g'=P.sum()/dw
           OK ths is a little complicated....
	   -   pass Y*(1/P)*(1/g^2)(g) to f'
	   -   pass Y*(1/P)*(1/g^2)(-f) to g'
	   -   sum results
   - next focus is .... pass to Sum
     	  = sum results


TODO: also return partial(output,input)

- should I make the theano-free version a theano "operator"?
- should I arrange that parameters can be theano shared variables?

api:
   tlog.infer("p(i,o)", input=X, params={'weight':W})
or f = tlog.inferenceFunction("p(i,o)", params={'weights':W})
   f(X,weights=W)

----------
Learning/training:
 - textcat toy w/o incoming chains DONE
 -- data/theory prep DONE
 -- matrixdb wrapper around theano shared variables DONE
 --- db contains a parameterDB sub-object, to which it delegates matrix and matrixpreimage requests
 --- matrixpreimage, matrix requests must include a context, eval or expression
     and returns shared.get_value() or just shared
 --- parameterDB also returns a set of parameters
 -- weighted facts/matrices - not needed yet
 -- trylearn.py following logreg.py example....

 - some KBC task
 - some binary predicate tuning task

Simple TODO's:
 - add a proof_failed constant so I never get a zero vector?
 - verify tensorlog works on matrix input, not just single rows
 - tensorlog CLI
 - cache matrixdb transposes and preimages
 - program.maxdepth parameter
 - test sparsity of theano stuff --- why is it so slow? is it actually dense?
 - scalability test - compare to real prolog?


Bugs:

- p(X,X) isn't handled correctly.  should I split j=0 into (0,'i') and
(0,'o') in the factor graph?  should there actually be 'factors' and
not goals?
 - 

Think thru: 
 - Blocks and Fuel
 - embeddings 
 - unary predicates or binary predicates with a constant (what's the inferred mode for those?)
 - pairs: possibly impossible to do as a gradient.  (note that for classification all I need is
   pair(X,c,Z) which could be done by just constructing the proper matrix in matrixdb),
   or even with pair_with_c(X,Z)
  - oneHot analysis
  - for theano, do envir's need to include some sort of prefix to keep the var namespaces apart?

------------------------------------------------------------------------------
