Minibatches: I should just give up and do the scan in the loss function level
 http://deeplearning.net/software/theano/library/scan.html --- except that gives up on
 using bricks, I guess?

Learning/training:
 - textcat toy w/o incoming chains DONE
 -- data/theory prep DONE
 -- matrixdb wrapper around theano shared variables DONE
 --- db contains a parameterDB sub-object, to which it delegates matrix and matrixpreimage requests
 --- matrixpreimage, matrix requests must include a context, eval or expression
     and returns shared.get_value() or just shared
 --- parameterDB also returns a set of parameters
 -- weighted facts/matrices - not needed yet
 -- trylearn.py following logreg.py example....

 - some KBC task
 - some binary predicate tuning task

Simple TODO's:
 - add a proof_failed constant so I never get a zero vector?
 - verify tensorlog works on matrix input, not just single rows
 - tensorlog CLI
 - cache matrixdb transposes and preimages
 - program.maxdepth parameter
 - test sparsity of theano stuff --- why is it so slow? is it actually dense?
 - scalability test - compare to real prolog?


Bugs:

- p(X,X) isn't handled correctly.  should I split j=0 into (0,'i') and
(0,'o') in the factor graph?  should there actually be 'factors' and
not goals?
 - 

Think thru: 
 - Blocks and Fuel
 - embeddings 
 - unary predicates or binary predicates with a constant (what's the inferred mode for those?)
 - pairs: possibly impossible to do as a gradient.  (note that for classification all I need is
   pair(X,c,Z) which could be done by just constructing the proper matrix in matrixdb),
   or even with pair_with_c(X,Z)
  - oneHot analysis
  - for theano, do envir's need to include some sort of prefix to keep the var namespaces apart?

------------------------------------------------------------------------------
