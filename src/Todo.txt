TODO: 

 - add to database loader code to handle ruleids
 - think about typed predicates and dense matrices
 -- maybe extend env so that env.matrixType[v] indicates type: sparse or dense+offset
 -- have ops dispatch to the correct operator as appropriate

 - try multithreading for minibatches
 -- maybe will need to recode broadcastAndWeightByRowSum,broadcastAndComponentwiseMultiply,softmax in pyrex/c?
 -- fun.backprop doesn't seem to be thread-safe: eg self.delta

 - optimization for cora-expt:
 -- train is 8.7s, mostly gradient
 -- matrix init is 3.7s
 -- mat mul is 2.5s
 -- broadcastAndWeightByRowSum is 2s (can densify)
 -- broadcastAndComponentwiseMultiply is 2.2s (densified)
 -- softmax is 1.9s (can densify)
 -- broadcastAndWeightByRowSum is also about 25%
 -- broadcastAndComponentwiseMultiply is 30%
 -- rather than types, I could just read in all constants in order so that
 you can group similar-typed ones together (eg, have 'foo.order' be 
 an extra input in programFiles and add a bit to the matrixdb to 
 loadConstants)

TUTORIAL/DOCS

  see wiki

Trip work:

 - spent time stabilizing the learner on grid task.  progress but
 learning rate still needs to be tuned by grid size....problems mostly
 were lack of regularization and exploding gradients
  - no regularizer arguments for expt yet TODO
 - learner now has regularizer and clips gradients and parameters correctly
 - major cleanup of mutil
 - regularization seems slow for wnet task compared to previously....should try with sparse params
 - explored using Log before SoftMax to make it more stable but experiments were disappointing, learning
 seemed much much slower.....
 - wrote debug.py
 - refactored preimages
 - tested sparse weight vectors - seem to work

TODO: Thinking thru functions and re-use: I'm not sure that re-used functions are handled properly.
They cache their outputs so if they are called multiple times at the same level with different
inputs we really need a copy of each instance. Proposed test case:

 	 p(X,Y) :- q(X,Y),r(Y,Z),r(X,W).
	 q(a,b)
	 r(X,Y) :- s(X,Y).  # so it's a re-used function
	 s(a,c). s(b,c).

BUGS: 

 - head p(X,X) doesn't work

DESIGN FLAWS: 1 fails but 2 is ok even tho both are polytrees.

 1 p(X,Y) :- q(X,X1),r(Y,Y1),s(X1,Y1)
 2 p(X,Y) :- q(X,X1),s(X1,Y1),r(Y,Y1)

EXTENSIONS:
 - throw sensible error when a novel rule constant is seen

config:
 - might want to include, in config.py, an overall 'top' configuration
 which includes the others as subconfigurations.  also might want to
 serialize/deserialize them.

learning:
 - serializable multi-task datasets
  propprExamplesAsData(db,fileName): convert to provide (xdict,ydict) outputs
 - adagrad & holdout set for convergence

MORE EXPERIMENTS:

 - Rose's representation learning/recommendation
 - SSL

