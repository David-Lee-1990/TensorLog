Tensorlog: A Differentiable Deductive Database

Intro

 - DDB system where DB encoded as sparse matrices, and process of
 query-answering can be compiled to a differentiable sequence of
 matrix-vector operatons.  Allows learning, also fast.  Potentially
 can be integrated with deep learning methods, allowing, eg,
 deep-learned UDF's or DDB-style access to external knowledge in a
 deep-learning system.

Preliminaries: 
- LP/DDB
 -- DB: predicate, constant, ground atomic literal 
 -- Program: logical variable, atomic literal, Horn/definite clause 
 -- Semantics: Proof, Herbrand base, interpretation, model, least model
 -- queries: bound variables, free variables 
- Factor graphs to represent a MRF
 -- random variable
 -- factor 
 -- joint distribution
 -- conditional prob: queries: evidence variable, query variable, noise variable
 -- BP and properties (exact for polytree)
 -- counting interpretation of BP
- Probabilistic LP/DDB
 -- random var per element of Herbrand base
 -- definition of Prob(interpretation)
 -- joint probability and queries
 -- answering queries: restrict to atomic formula that are involved in a proof for Q
 -- counting interpretation of semantics: want weighted proof-counting

Tensorlog:
 - definition: 
 -- just as above + notation: {ruleId}
 --- restriction: each predicate used only once in a proof
 --- restriction: binary predicates, each clause is a polyforest
 - inference
 -- query => predicate io-mode 
 -- predicate io-mode => factor graph and BP message sequence (operators)
 --- details on messages => operator sequence
 -- normalization and Normalize function
 - extensions
 -- multiple clauses and Sum function
 -- recursion, depth and NullFunction
 -- PSEUDO nodes for ruleId and ruleFeatures
 - learning
 -- differentiability
 -- clipexp for predicate weights?
 -- optimization
 - implementation notes

Related work
 - ProbLog, MLNs, prob DDB, ICL
 - SLPs, ProPPR
 - FactorIE?
 - Probabilistic LP: surveys (Kersting & De Readt, Roth)
 -- approach 1: MLNs, PSL. 
 --- one binary random variable per atomic formula
 --- factors between random variables that produce a distribution (eg, ground clause)
 --- downside: size of graph, inference cost for MLNs
 - approach 2: restrict to probabilistic inference deductive closure of theory: ProbLog, SLPs, ProPPR
 -- still one binary random variable per atomic formula
 --- used in SLP and ProPPR to define joint distribution indirectly
 --- used in ProbLog and Sucio's probabilistic DBs (cite) to compute probabilities more directly
 -- downside: need to explore full space of proofs, since typically all proofs influence truth/falsity

Experimental results
 - Execution time vs Prolog, ProPPR: KBC problems
 - Learning vs ProPPR

Extensions and future work
 - Integration with ANNs
 -- as inputs/outputs in joint learner
 -- as declarative spec of ANN
 -- into Theano, etc
 -- typing, GPUs and parallelism
