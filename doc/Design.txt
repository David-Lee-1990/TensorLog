Theano-embeddable variant of PROPPR.
--------------------

Top-level 

tensorlog.py
  top-level
  
matrixdb.py
 load/save matrices

builtin.py
  can claim any goal/mode combination, eg pair(i1,i,o), pair(i,i1,o)

analysis.py: 
  analyzes modes for clauses 
  uses MatrixDB, BuiltinOps, and TensorlogPrograms to convert a mode + set of rules to TensorCode 

TensorCode: always has inputs,outputs, parameters?
 Sequence: + list of operations 
   Operations:
     RightMatMul(Yr = Xr * M_p, transpose=true/false)
     ComponentWiseProd(Yr = Xr * diag(C) )
     LeftMatMul(Yc = M_p * Xc, transpose=true/false)
     FunctionCall(Y = f(X), f defined by goal and mode)
 Sum: + list of Sequences - for proof-counting version of code
 PropprPred: + list of pairs ( Y=f(X), F=g(X) ),
   first gives output
   second gives features
   weighting scheme is implicitly ...

theanocompiler.py - converts Tensorcode to Theano

----------
tensorlog.py
 like proppr syntax except rule is A :- B1,...,Bk { F : C1,..,Ck }.
 eg classify(X,Y) :- { F : hasWord(X,W),wordLabelPair(W,Y,F) }

 tl.Mode: specific mode for a specific goal in context 
   mode.input(i)
   mode.output(i)
   mode.onehot(i)
  	
 tl.ModeDeclaration: possible modes for a predicate to be used
   specified by: goal or unparsed goal string like: p(i,o), q(i1,o1), p(u,u)
   
 tl.TypeDeclaration: types of a predicate like: hasWord(doc,word)

 tl.Database: 
   
   db.M(goal,mode,direction): returns matrix for a goal for matops or None
    # or maybe db.op(...): returns operator?
   db.fun(goal,mode,direction): returns some sort of function-creator (FunOp?) or None
   db.load(dir): ... # can this all be done upfront, before program analysis, or not?
   ...
   db.definePair(type1,type2)
 
 tl.Program:  #should this be a Bricks 'model' ???
   modeDeclarations = [ ... ]  # list of strings
   database = directory # file or string
   rules = [...] # list of strings 
 
   p.function( modeDeclaration ): returns a pair (TheanoFunction,Params)


notes on converting horn clauses to mat multiplication.  assume
unary/binary clauses for now

class VarInfo:
 outputOf = goalId [then it's bound for goals j>i]
 onehot = bound to a singleton set
 inputTo = list of goalId's

class goalInfo:
 inputs = set of vars that are inputs 
   (simple case is only one, but could be multiple onehot inputs, maybe, say for pair
 outputs: set of vars that are outputs
 rootAncestors: set of goals that (transitively) provide inputs
   (if this is empty it's a root)
 direction: used for forward or backward propagation
 mode: inferred mode as used: for each arg, is it i/o, onehot or not

Analysis: 

 - graph must be a polytree, with one root being the input variables
of the lhs goal (as determined by its mode declaration)

 - the main chain is the single path (since it's a polytree) from
   lhs.in to lhs.out. find this by walking back from goal with output
   variable = lhs output variable to lhs, always picking the ancestor with
   the lhs as a rootAncestor

 - other chains are incoming or outgoing.

 -- incoming chains: find all solutions to their output variables with
    "real" prolog and build a no-incoming-chain version of the clause
    for each input.

 -- outgoing chains: want to count the number of solutions (paths to
    end of chain) for each possible input value.  If the binary
    matrices are Mq1, ..., Mqk, and ones is an all-ones row vector,
    then this is: a column vector: vi = Mq1(Mq2(....(Mqk ones^T))),
    which is multiplied with the corresponding vector variable on the
    main chain.

Algorithm:
 - find polytree
 - load the incoming-chain predicate matrices, and compile out incoming chains, so we have a tree
 - mark the tree predicates by mode, and load in all the needed matrices
 - compile the tree:
 -- for each with a branch off the main tree, recursively find the vector for that
    node, and multiply it in at that point.

    -- more specifically, we'll generate code to compute the forward
    and backward counts for each variable.  forward propagation is
    only on the main chain.  backward propagation is across the whole
    tree. We'll define a backward column vector for each goal and each
    variable (maybe the notation should be row_v and col_v for forward
    and backward?)

      back[ j ] = M_p * back[Y] where j points to p(+X,-Y)
      back[ j ] = c_p where j points to unary p(+X)
      back[ V ] = componentWiseProduct_{j in V.inputTo} back[j]
        special cases: 
	- if inputTo==[] then back[V] = ones.T
	- if inputTo==[j] then back[V] = back[j]
	- otherwise generate: { bv_0 = back[j0]; bv_1 = bv_0 * diag(back[j_0]); .. bv_k = bv_{i-1}*diag(back[j_k]); back[v] = bv_k }
		  
   -- I *think* we want to propagate backward from the side-chain
  leaves, and forward from the lhs inputs, replacing the forward score
  with forward*backward for variables with both main-chain and
  side-chain consumers. algorithmically, generate the back's first,
  recursively since the graph is a tree, over the non-mainchain goals.
  then go down the mainchain and if back[v] exists multiply it with v
  to get rc_v before using rc_v as input in the chain.


Extensions:

 - onehot inputs: you can have multiple onehot inputs to the LHS
 (that's sort of what happens when you compile out the incoming chains
 into constants), and they should all count as being in the same
 equivalence class.  onehot's derived from these are also in the same
 class: conceptually you could compute all these functions from the
 inputs, so combining these in any way doesn't cause flow problems.
 so: the lhs inputs should be a set, not a single variable, for
 analysis purposes.

 - allow n-ary predicates that are binary after compilation?
 
 i(P,X,Y) :- rel(R,X,Z),rel(Q,Z,Y)  doesn't work
 try:
 i(P,X,Y) :- rel(R),rel(Q),rel(R,X,Z),rel(Q,Z,Y) { f(PQR) : tuple(P,Q,R,PQR) }
 forall r,q:rel(r),rel(q): i(P,X,Y) :- rel(r,X,Z),rel(q,Z,Y)

 i(P,X,Y) :- rel(R),rel(R,X,Z),rel(Z,QY),a2(QY,Y).  
 forall r,q:rel(r): i(P,X,Y) :- rel(r,X,Z),rel(Z,QY),a2(QY,Y),a1(QY,Q),tag(P,r,Q). 
 #might be ok? P is 1hot, r is constant, Q is set of all things which can be 
 #chained together with r to get 
 tag(P,R,Q) :- { f(PRQ) : tuple(P,R,Q,PRQ) }

--------------------

Semantics:
 - Herbrand base will be integers 0..N
 - base is created when graph relations are loaded
 - each i has a 'type' type(i), which is inherited from the column it
   occurs in.  (it's an error if i is in more than one type.)

 - the set/distribution of bindings for X will be a sparse row vector v_X, in general defining { x : x in X}

 - semantics for a clause body B1,...,Bk:
 -- restrict this so that the join structure is a chain. (i.e., each predicate has one bound input.
 We could allow predicates with multiple bound inputs, but we'd want to mark them somehow
 as special since the sets they take are marginals.)

 --- remember everything is a multi-set: so p(+X,-Y) really means:
    "let Y = { y : p(x,y) true for some x in X }"
 --- specifically: v_Y[j] = |{ tuples(x,j) in p : x in X }|
 --- the chain is thus: "let Yk = { yk : p(x,y1) and .. p(y_k-1, y_k) true for some x in X }"
 --- specifically: v_Yk[j] = |{ tuples(x,y1,..,j) in join(p1,...,pk) : x in X }|
 -- now consider projecting out a subset of the variables (including X)

 example: p1(X,Y1),p2(Y1,Y2)  - join is tuples <x,y1,y2>
 for any y1, the number of solutions including y1=j is v_Y1[j] * ... ? 
 this is something like forward-backward: we need the number of paths from j to the end.
 special case is going all the way back to X

 -- similar semantics work for a tree rooted at X
 -- can extend to handle k-ary predicates if they are always indexed appropriately
 -- can handle unary predicates as just multipliers/diagonal matrices

Hard eg: "predict(X,Y) :- isLabel(Y),classify(X,Y)" isn't a chain or
tree, since classify(X,Y) has TWO inputs.  So putting X in the middle
of a chain can't work because we don't have any pairwise counts.

We could do the MLN thing and duplicate things (or maybe use "scans"
in Theano instead?) Note: this fails for the WSD 'propose' case,
though, unless we can somehow be clever about when to expand the
proposed 'Ys'.  (We could do this automatically, I guess, for
existentially-quantified stuff not dependent on X.)

classifier:
  for each y in Y:
   classify(X,y) :- { pair(W,y): hasFeature(X,W) }

entreg: for all unlabeled x require -fail(x)
  for each y1,y2 in Mutexes:
    fail(X) :- classify(X,y1),classify(X,y2)

cotrain: for all unlabeled x require -fail(x)
  for each y1,y2 in Mutexes:  
    fail(X) :- classify1(X,y1),classify2(X,y2)

lagrangian reg: for all edges u,v require -fail(u,v)
  for each y1,y2 in mutexes:
    fail(U,V) :- classify(U,y1),classify(V,y2)

mrw reverse: 
  predict(Y,X) :- seed(Y,X).
  predict(Y,X) :- seed(Y,Z1),edge(Z1,X).
  predict(Y,X) :- seed(Y,Z1),edge(Z1,Z2),edge(Z2,X).
  ...

mrw forward:

  predict(X,Y) :- seed(X,Y)
  predict(X,Y) :- edge(X,Z1),seed(Z1,Y).
  ...

coupled learning of relations and types

  classify(U,c) :- ...
  classify(UV,r) :- ...
  fail(UV) :- classify(UV,r),domain(R,C),mutex(C,C1),arg1(UV,U),classify(U,C1).

Structure learning:

  Externally learned PRA rules are ok.

  Structure learning seems impossible, because we can't collect the
  interacting sets of variables with an abductive rule

    # P(X,Y) :- R(X,Z),Q(Z,Y)
    i(P,X,Y) :- rel(X, (R,Z)), rel(Z,(Q,Y)), ab_chain(P,Q,R)

    ab_chain(P,Q,R) :- ... 

  at this point, what we have is sets of Qs and Rs that link X to Z and
  Z to Y, but we've lost interaction information.  maybe that's ok?

    ab_chain(P,Q,R) :- { code to generate triple features of p,q,r where p in P,q in Q,r in R? }

  Of course there could be too many triples to handle anyway....

Recommendation:

old:
  likes(U,I) :- predictCluster_u(U,Ku),predictCluster_i(I,Ki),ab_likes(Ku,Ki).
  predictCluster_u(U,K) :- pickCluster_u(K),classifyCluster_u(U,K).
  classifyCluster_u(U,K) :- { f1(U,K) }.
  ...
  # similar for _i

new: somehow specify that embedding/2,user2Latent/2,latent2Item/2 are
parameters, and train 

  rating(U,Ru) :- embedding(U,Eu),user2Latent(Eu,L),latent2Item(L,Ru)

Hidden layers and such  TODO

 predict(X,Y) :- input(X,XI),link(XI,H),link(H,Y).
 link(I,J) :- { w(I,J) } -- fails since these are now sets


====================

ALSO:

 -- we could keep track of whether this is a singleton set or not
 - a chain of binary relations gets compiled to a series of assignments
 --- output of Bk for var Y is Yk, which I'll denote oY below
 --- input for Bk for var X is max_{i<k} Xi, which I'll just denote iX below
 ---- X0 is input from the clause head
 - p(+X,-Y) for proppr graphs: create a matrix M_p and assign v_oY = v_iX * M_p

I guess these can all just be: vectors as Theano shared vars,
assignments as Theano assignments, ...

Body goals:
 - body goals are restricted:
 -- p(-Y,-Z): should be possible somehow...
 -- arg1(+X,-Y), arg2(+X,-Y): again, pre-computed matrices
 - can also plug in other built-ins as needed
 - all builtins can access the parameters Theta

Features:

 - features: features of a clause A:-B1,...,Bk are replaced with a
   function w(v_X,Theta) of the inputs of the clause and the
   parameters Theta of the program
 - Given a functions f_ri(v_X) => v_Y for rules r1,...,rK,
  the function for a predicate definition is
 
  f(v_X) = 1/z ( v_Y*w_i(v_X,Theta) + v_Y*w_K(v_X,Theta) )

  where 1/z is sum_i w_i(v_x,Thera)

Loss:

 - Data would be matrix X,Y plus a loss function between the learned f(X,Theta)=Y^ and Y

==============================================================================

Program: 
  Theta
  { p1+mode : predDef1 }
  interp(p+mode, X)=> Y

Interpreter: [program, p+mode, X] ==> Y

Background predicates and types
 - type signatures: p +type1 -type2 [default signatures p: +const -const]
 - files to load: f1.graph ....
 => matrix dict M={p1:Mp1, ...}
 => storable as matlog.m/p1.dat, ....

TrainingTask:
  TrainingData: set of (p,X,Y*) triples - X and Y* are matrixes
  Loss function: L(interpreter, TrainingData)


------------------------------------------------------------------------------
DEAD ENDS: 

Notes: these don't work since X's are all bags of instances
 -- INCORRECT! eq(+X1,+X2): assign v_oX1 = v_oX2 = v_iX1 o v_iX2   [o = component-wise product]
 -- INCORRECT! p(+X1,+X2): for proppr graphs: v_oX1 = v_oX2 = (v_iX1 * M_p) o v_iX2   [o = component-wise product]
 -- INCORRECT! p(-Y): assign v_oY = v_p
 - when we construct pair mappings then we do it for all elements of the given types

