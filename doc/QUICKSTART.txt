BASICS

A Tensorlog DATABASE is holds a bunch of unary and binary relations,
which are encoded as scipy sparse matrixes.  The human-readable format
for this is a set of files with the .cfacts extension.  Some examples,
from tensorlog/test-data/textcattoy.cfacts:

 hasWord	dh	a
 hasWord	dh	pricy
 hasWord	dh	doll
 hasWord	dh	house
 hasWord	ft	a
 hasWord	ft	little
 hasWord	ft	red
 hasWord	ft	fire
 hasWord	ft	truck
 ...
 label	pos	
 label	neg

The columns are: predicate name, argument 1, and (optionally, for
binary predicates) argument 2.  An additional column can be added
which is a numeric weight (so don't use any constant that parses to a
number in a cfacts file to avoid program confusion - or if you do then
set matrixdb.conf.allow_weighted_tuples = False.)  You need to group
facts with the same predicate together.

Typing: You can optionally add type declarations in a cfacts file,
like this:

# :- predict(doc,label)
# :- hasWord(doc,word)
# :- posPair(word,labelWordPair)
# :- label(label)

This will basically put the constants of type 'doc', 'label', etc in
different namespaces.  Types are all disjoint. You should either type
everything or nothing (in the latter case, everything is a default
type __THING__).

You can also mark a DB predicate as a parameter with a declaration with
a line like

# :- trainable(posWeight,1)

A database can be SERIALIZED and should be stored in a directory with
extension .db.

A Tensorlog PROGRAM usually has extension .ppr.  Some examples:

------------------------------------------------------------------------------
 predict(X,Pos) :- assign(Pos,pos) {pos_weight(F): hasWord(X,W),posPair(W,F)}.
 predict(X,Neg) :- assign(Neg,neg) {neg_weight(F): hasWord(X,W),negPair(W,F)}.
 predict(X,Y) :- classify(X,Y) {weight(Y): true}.
 
 match(R,S) :- fname(R,FR),fmatch(FR,FS),fname(S,FS) {f}.
 match(R,S) :- lname(R,LR),lmatch(LR,LS),lname(S,LS) {l}.
 match(R,S) :- addr(R,AR),amatch(AR,AS),addr(S,AS) {a}.
------------------------------------------------------------------------------

Semantics: The first clause above is converted to

 predict(X,Neg) :- assign(Neg,neg), hasWord(X,W),negPair(W,F),pos_weight(F).

but the {} syntax makes it more obvious what is used for 'control'.
The third clause is converted to the following ("true" is a special
dummy literal) here:

 predict(X,Y) :- classify(X,Y), weight(Y).

The last clause is converted to the following ("weighted" is a special
predicate.)

 match(R,S) :- addr(R,AR),amatch(AR,AS),addr(S,AS), assign(RuleID,a), weighted(RuleID).

Typing: you can use assign(Var,const,type) to if you're using types: eg,

 predict(X,Pos) :- assign(Pos,pos,label) {all(F): hasWord(X,W),posPair(W,F)}.

If you use the ProPPR-style rule features (in the curly braces) you
should 
 1) make sure any constants appearing there are in the database.
Pro tip: If you make these all appear in the database as arguments to the
unary predicate 'weighted' then program.setRuleWeights() will
operate properly by default.  If you use another unary predicate
you need to specify it in as an argument to program.setRuleWeights.
 2) Load the rule file as 'proppr' format, which is NOT the default.

There's no serialized form of a program.  

There's a more Pythonic syntax for rules, which can be used to create
rules programmatically, described in the docs for
tensorlog.simple.Builder class.  Briefly, some examples are:

    from tensorlog import simple			     
    b = simple.Builder()
    X,Y,Z = b.variables("X Y Z")
    aunt,parent,sister,wife = b.predicates("aunt parent sister wife")
    uncle = b.predicate("uncle")
    b.rules += aunt(X,Y) <= parent(X,Z),sister(Z,Y)
    b.rule += aunt(X,Y) <= uncle(X,Z),wife(Z,Y)

Or, with 'control' on the rules:

    r1,r2 = b.rule_ids("r1 r2")
    ...
    b.rules += aunt(X,Y) <= uncle(X,Z) & wife(Z,Y) // r1
    b.rules += aunt(X,Y) <= parent(X,Z) & sister(Z,Y) // r2
    b.rules += aunt(X,Y) <= uncle(X,Z) & wife(Z,Y) // (weight(F) | description(X,D) & feature(X,F))

