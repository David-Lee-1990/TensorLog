\documentclass[12pt]{article}

\usepackage{graphicx}

\newcommand{\yy}[1]{\textit{TODO: {#1}}}
\newcommand{\ct}[1]{\textit{cite: {#1}}}
\newcommand{\cd}[1]{{\small \texttt{#1}}}

\newcommand{\trm}[1]{\textit{#1}}
\newcommand{\vek}[1]{\textbf{#1}}
\newcommand{\M}{\textbf{M}}
\newcommand{\support}[1]{\textit{sup({#1})}}
\newcommand{\children}{\textit{children}}

\title{TensorLog: A Differentiable \\
Deductive Database System}

\begin{document} 

\maketitle

\begin{abstract}
We describe a restricted deductive database language where relations
must be binary, but can be weighted facts.  In our setting the data
can be encoded as a set of sparse matrices, and algorithm used for
query-answering can be compiled into a differentiable sequence of
matrix-vector operations.  This allows learning: one can impose a cost
function which penalizes disagreement with training-data queries,
differentiate the cost function with respect to the weights on the
facts, and update the database.  \yy{discuss results, generality,
  potential for integration with deep-learning: eg, deep-learned UDF's
  or DDB-style access to external knowledge in a deep-learning
  system.}
\end{abstract}


\section{Introduction}

\yy{todo}

\section{Preliminaries}

\subsection{Logic programs and deductive databases}

\yy{todo - define terms}

We will assume some familiarity with logic programs.  Let ${\cal T}$
be the theory (aka ``intensional database'', ``rules'') of a logic
program, consisting of Horn (aka definite) clauses, which will be
written $A\leftarrow{}B$ or $A\leftarrow{}B_1,\ldots,B_\ell$.  We will
use $p,q,\ldots$ for predicate symbols, uppercase symbols like $X$ and
$Y$ for logical variables, and letters like $a$, $b$, $c$ for
constants.

The other part of the logic program is a database ${\cal DB}$ of unit
clauses, aka facts.  We assume a fixed domain of constant symbols
${\cal C}$ that appear these facts.

\yy{example: uncle}

\yy{todo 
 -- DB: predicate, constant, ground atomic literal 
 -- Program: logical variable, atomic literal, Horn/definite clause 
 -- Semantics: Proof, Herbrand base, interpretation, model, least model
 -- queries: bound variables, free variables 
}

\subsection{Markov random fields and factor graphs}

\yy{todo:
- Factor graphs to represent a MRF
 -- random variable
 -- factor 
 -- joint distribution
 -- conditional prob: queries: evidence variable, query variable, noise variable
 -- BP and properties (exact for polytree)
 -- counting interpretation of BP
}

\subsection{Probabilistic deductive DBs}

\yy{todo:
- Probabilistic LP/DDB
 -- random var per element of Herbrand base
 -- definition of Prob(interpretation)
 -- joint probability and queries
 -- answering queries: restrict to atomic formula that are involved in a proof for Q
 -- counting interpretation of semantics: want weighted proof-counting
}

\section{Inference in TensorLog}

\subsection{Intuitions and Examples}

\subsubsection{Factor graphs for logical inference}

Let us assume that all the constants in the DB have been mapped to
integers (so below, $a,b,\ldots$ refer to integers as well as constant
symbols).  A binary DB predicate defined by the facts
$p(a_1,b_1),\ldots,p(a_n,b_n)$ can also be encoded with a sparse
matrix $\M_p$ defined as:
\[
  \M_p[a,b] \equiv \left \{
   \begin{array}{ll}
     1 & p(a,b) \in {\cal DB} \\
     0 & \mbox{else} \\
   \end{array}
   \right.
\]
Clearly, allowing the elements of $\M_p$ to have positive values
different than 1 allows us to encoded a weighted set of facts, and
likewise a unary predicate $q$ can be encoded as a row vector (as can
any weighted set $S$ of constants) and a constant $a$ can be encoded
as with its ``one-hot representation'', i.e., the vector $\vek{u}_a$
where
\[
  \vek{u}_a[i] \equiv \left \{
   \begin{array}{ll}
     1 & \mbox{$i=a$} \\
     0 & \mbox{else} \\
   \end{array}
   \right.
\]

For now, however, let us consider this
simple example: we are given DB predicates for \texttt{parent} and
\texttt{brother}, and a theory ${\cal T}$ which contains the single
clause
\[ \mbox{\texttt{uncle(X,Y):-parent(X,Z),brother(Z,Y)}}
\]
One possible mechanism for answering queries would be to set up the
following linear-chain factor graph

\centerline{\includegraphics[width=0.8\textwidth]{./figures/chain.png}}

\noindent where $X$, $Y$, and $Z$ are multinomial random variables
whose domain is ${\cal C}$, the set of DB constants, and
\texttt{parent} and \texttt{brother} are factors, with potentials
given by precisely the matrices $\M_\texttt{parent}$ and
$\M_\texttt{brother}$.  In the distribution defined by this MRF
\[ \Pr(X=x,Z=z,Y=y) \propto \M_\texttt{parent}[x,z] \cdot \M_\texttt{brother}[z,y]
\]
and hence a non-zero value will be given to a triple $(x,y,z)$
precisely when \texttt{uncle(x,y)} can be proved in ${\cal T}$, and a
number of logical queries can be answered by probabilistic inference
on the factor graph. For instance, to answer the query
\texttt{uncle(al,Y)} we could use the model above to find constants
$y$ for which \( \Pr(Y=y|X=\texttt{al}) \) was non-zero.

The first key idea in TensorLog is simply to use this formulation for
logical reasoning---i.e., reasoning as BP in a factor graph where the
factors correspond to predicates and the random variables correspond
to logical variables in a Horn clause.

\subsubsection{Query Response Strategies based on a factor graph}

Further suppose that we only care about answering queries of the form
\texttt{uncle(a,Y)}, where $a$ is some constant.  The factor graph
above is simple enough that we can easily determine which messages
will be passed in belief propagation (BP). The steps are as follows:
\begin{enumerate}
\item To encode the evidence, encode $\Pr(X=x)$ with a vector
  $\vek{v}_1$, which is simply the one-hot encoding for $a$ above,
  and send that message to the factor for $\M_\texttt{parent}$.
\item From that factor, encode $\Pr(Z=z|X=x)$ with the vector
  $\vek{v}_2$, which can be computed with the vector-matrix product
  $\vek{v}_1 \M_\texttt{parent}$, and send that message to the node for
  $Z$.
\item Forward the message $\vek{v}_2$ to the factor for $\M_\texttt{brother}$.
\item From that factor, encode $\Pr(Y=y|Z=z)$ with the vector
  $\vek{v}_3 = \vek{v}_2 \M_\texttt{brother}$, and send that message to
  the node for $Y$.
\end{enumerate}

More generally, let a \textit{query type} be defined by a predict name
and a pattern of inputs/outputs (or, if you prefer, evidence and query
variables) as in the example \texttt{uncle(a,Y)} above.  If the BP
process for a factorgraph is simple enough, we can simulate it and
derive a \emph{fixed sequence of message-passing operations} that can
be used for all queries of this type.  We call this sequence of
operations a \textit{query response strategy (QRS)}.  

Note that the QRS in this example is a function: it takes
a one-hot encoding $\vek{u}_a$ and outputs the vector
\[ f_\texttt{uncle}(\vek{u}_a) \equiv \vek{u}_a \M_\texttt{parent} \M_\texttt{brother}
\]
This function, together with a final normalization step, is sufficient
to answer queries of one type. The second key idea in TensorLog is
simply to answer queries not directly by BP, but indirectly, by
constructing QRS functions.  The main benefit of this is that the
functions can be differentiated symbolically, allowing learning using
simple and scalable gradient methods.

\subsubsection{Road map of this section}

\yy{extending the example  
 - definition: 
 -- just as above + notation: {ruleId}
 --- restriction: each predicate used only once in a proof
 --- restriction: binary predicates, each clause is a polyforest
 - inference
 -- query => predicate io-mode 
 -- predicate io-mode => factor graph and BP message sequence (operators)
 --- details on messages => operator sequence
 -- normalization and Normalize function
 - extensions
 -- multiple clauses and Sum function
 -- recursion, depth and NullFunction
 -- PSEUDO nodes for ruleId and ruleFeatures
 - learning
 -- differentiability
 -- clipexp for predicate weights?
 -- optimization
 - implementation notes
}

\end{document} 
