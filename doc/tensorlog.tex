\documentclass[12pt]{article}

\newcommand{\yy}[1]{\textit{TODO: {#1}}}
\newcommand{\ct}[1]{\textit{cite: {#1}}}
\newcommand{\cd}[1]{{\small \texttt{#1}}}

\newcommand{\trm}[1]{\textit{#1}}
\newcommand{\vek}[1]{\textbf{#1}}
\newcommand{\M}{\textbf{M}}
\newcommand{\support}[1]{\textit{sup({#1})}}
\newcommand{\children}{\textit{children}}

\title{Differentiable Logic Programs: DRAFT}

\iffalse


INTRO

 - motivation/background.  H is Herbrand base, model is for Pr(phi in
 H|theory,DB).

 - approach:

 -- limit ourselves to arity 1 and two predicates.  usually we answer
 queries q of the form: r(x,Y) or r(Y,x), where r and x are given at
 query time. So the query response R involves only a small part H -
 that matching (unifying with) the query.  Further, we often only care
 about the relative scores of fmla in R: ie for \phi in R, we don't
 need Pr(\phi) according to Pr_H(phi) but 1/Z Pr_H(phi), where Z =
 Pr_H(R).

 -- so don't model a distribution over the Herbrand base. instead,
 consider specific families Q, and build ``response strategies'' that
 work for all q in Q.  Q will be relation + input/output pattern.  A
 RS for Q will return a distribution over the free variables.  For the
 binary single-input case, this is a distribution over bindings for
 the single output variable.  QRS is consistent with some Pr_H.

 - road map

QRS FOR ONE CLAUSE THEORIES

- algorithmic QRS based on BP

consider one clause theories and a proof-counting distribution. Then
QRS can be defined using BP over a graph where random vars are logical
vars and literals are factors - factor for r(X,Y) has # inputs =
arity, and #non-zero terms in factor just the number of facts for R.
[Examples] This is an algorithmic QRS.

- functional QRS

for any clause where graph is a forest of polytrees, BP will send a
deterministic sequence of messages. We can simulate BP and obtain this
message sequence. [Examples] This is an functional QRS: a known
function f:q->v_R that provides the response to query.  This
differentiable.

FUNCTIONAL QRS FOR MORE COMPLEX THEORIES

- multiple clauses for a predicate
- recursive theories, with recursion carried out to fixed depth
- weighted proof-counting

LEARNING

- adding a loss function
- can learn weights for any predicate 
- rule-weighting as in SLPs - syntactic sugar
- feature-based rule-weighting as in ProPPR - syntactic sugar



\fi


\begin{document} 

\section{Introduction}

Probabilistic first-order logics \ct{MLNs, PSL, ProbLog, SLPs, ProPPR,
  [first-order Bayes nets]...} model a joint distribution over ground
atomic formula in the Herbrand base of a logic program (LP).  In other
words, they generalize logic as follows: whereas an ordinary LP
assigns a binary value, true or false, to a set of potential beliefs,
a probabilistic LP defines a joint probability distribution over this
set of beliefs.  In the simple case the beliefs are ground atomic
formulae like \cd{uncle(al,chip)} or \cd{mother(bea,chip)},
and more generally are of the form $p(e_1,\ldots,e_k)$, where $p$ is a
predicate symbols and the $e_i$'s are constant symbols, typically
corresponding to entities in some domain of interest.  If the set of
all potential beliefs of this form is denoted $H$ (the ``Herbrand
base'') then the deductive closure of a LP is a particular subset of
$I\subset H$, which is interpreted as the beliefs that implied by the
program.  In contrast, probabilistic extensions to LPs can generally
be viewed as defining a joint distribution $D$ over a set of random
variables $X_1,\ldots,X_{|H|}$, where each $X_b$ corresponds to a
particular belief $b$.  This joint distribution allows probabilistic
inferences to be made: so rather than asking if
\cd{husband(al,bea)$\wedge$mother(bea,chip)} logically implies
\cd{uncle(al,chip)}, one would answer probabilistic queries like
\[ Pr_D\left (X_\cd{uncle(al,chip)} | X_\cd{husband(al,bea)} \wedge X_\cd{mother(bea,chip)} \right)
\] 
There are number of ways that this distribution $D$ can be defined:
for instance, MLNs can be thought of as a factor graph where there is
one binary variable for each belief, and a factor for each ground
instance of a clause. \yy{add example to fig?}  

The various types of probabilistic LPs adopt a number of schemes for
defining $D$, which offer different tradeoffs in efficiency and
generality.  For example, PSL \ct{...} replaces the Markov-field
inference of MLNs with a more tractible optimization that is based on
a hinge loss.  Other logics \ct{ProbLog, suciu's} do not construct all
variables $X_b$ explicitly, but instead answer probabilistic
inferences by manipulating the set of logical derivations supported by
a program, with stochastic LPs \ct{...} and ProPPR \ct{...} actually
defining $D$ indirectly in terms of the space of derivations.  While
much progress has been made, inference remains difficult, in part
because of the huge size of the set of random variables. For instance,
in a task like knowledge base completion (KBC), the set of potential
beliefs $H$ is certainly larger than the size of the knowledge base
(KB), i.e., the set of known ground facts, in realistic KBC problems
the KB will contain hundreds of millions of facts.

We propose here another way of extending LPs to probabilities.  Rather
than introducing a new binary random variable $X_b$ for each potential
belief $b$, we introduce a new random variable $X_v$ for each logical
variable $v$ in a program, where the domain of the random variable is
the set of \emph{constant symbols} appearing in the Herbrand base.  We
show that a large class of logic programs can be converted to factor
graphs, that belief propagation (BP) can be used to perform exact
inference, and that this BP inference is a strict generalization of
logical inference on the original LP.  

\yy{example figure and discussion}

For the class of LPs we will consider, the factor graph that is
constructed is a polytree, so BP is exact, and converges with a finite
number of messages.  In fact, given a known query type (which
corresponds to a particular input-output mode for a known predicate),
one can easily construct the exact sequence of messages that will be
sent by the BP system Inference in this setting can be viewed as
computing a function, the function being defined by the particular
message sequence constructed.  \yy{example}.  The consequence of this
is that query-answering in our probabilistic LP system can be reduced
to computing a function, which is in fact a \emph{differentiable}
series of matrix-vector operations.  Using deep learning methods on
this function allows a wide range of learning tasks to be performed,
and can potentially enable a wide range of possibilities for
integration between LPs and neural networks.

Below we will describe in detail the subset of logic included in
TensorLog, and how that subset can be converted into factor graphs.
We will show that the resulting inference does in fact generalize
logic.  \yy{discuss relation to ProPPR and SLPs}

\section{Details}

\subsection{Notation and goals}

${\cal T}$ is the intensional part of a logic program, consisting of Horn
clauses, written $A\leftarrow{}B$ or $A\leftarrow{}B_1,\ldots,B_\ell$.
In literals, I will use $p,q,\ldots$ for predicate symbols, variables
like $X$ for inputs and $Y$ for outputs, and letters like $a$, $b$,
$c$ for constants.

The other part of the logic program is a database ${\cal DB}$ or unit
clauses, aka facts.  I assume a fixed domain of constant symbols in
these facts, which I will assume have been mapped to integers (so
$a,b,\ldots$ also refer to integers.)  For a binary DB predicate
defined by the facts $p(a_1,b_1),\ldots,p(a_n,b_n)$ there is a
corresponding sparse matrix $\M_p$ defined as
\[
  \M_p[a,b] \equiv \left \{
   \begin{array}{ll}
     1 & p(a,b) \in {\cal DB} \\
     0 & \mbox{else} \\
   \end{array}
   \right.
\]
For a unary predicate $q$, we will define a row vector $\vek{c}_q$ as
follows
\[
  \vek{c}_q[a] \equiv \left \{
   \begin{array}{ll}
     1 & q(a) \in {\cal DB} \\
     0 & \mbox{else} \\
   \end{array}
   \right.
\]
Likewise we will define for each $a$ a unit row vector $\vek{u}_a$,
aka the one-hot representation representation for $a$, as a row vector
in which
\[
  \vek{u}_a[i] \equiv \left \{
   \begin{array}{ll}
     1 & \mbox{$i=a$} \\
     0 & \mbox{else} \\
   \end{array}
   \right.
\]
Similarly any set $S=\{a_1,\ldots,a_k\}$ could be encoded as a row
vector 
\[ \vek{u}_S \equiv \vek{u}_{a_1} + \ldots + \vek{u}_{a_k}
\]
For any vector $\vek{v}$ we define the \trm{support} of $\vek{v}$,
written $\support{\vek{v}}$, to be the set of non-zero indices, and the
weight of $a \in \support{\vek{v}}$ to be the value if $\vek{v}[a]$.

A \trm{mode} ${\cal M}$ is a literal with arguments that are the
constants $i$ or $o$, for input and output: e.g., $p(i,o)$ means that
the first argument for predicate $p$ will be an input.

The plan is to define a way in which ${\cal T,DB}$ plus a mode ${\cal
  M}$ can be interpreted as a function $f_{\cal M}$, so that the
function can be implemented solely with differentiable matrix
operations.  The function will be defined more-or-less the ``obvious''
way: for instance for ${\cal M}=p(i,o)$, the function $f_{\cal M}$
will be
\[ f_{\cal M}(a) \equiv \{ b : p(a,b) \mbox{~is entailed by~} {\cal T,DB} \}
\]
The ``less'' part of the ``more or less'' is that the actual output of
$f_{\cal M}(a)$ will be a row vector $\vek{v}$ so that 
\[ \support{\vek{v}} = \{ b : p(a,b) \mbox{~is entailed by~} {\cal T,DB} \}
\]
and furthermore, the weight of $b$ in $\vek{v}$ will be the number of
proofs that entail $p(a,b)$.

\yy{need to rewrite this - maybe just examples}

\subsection{Case 1: A single chain with the final variable as output}

We will start by assuming that ${\cal T,DB}$ contains only binary and
unary predicates, and that ${\cal T}$ is a simple ``chain join'' of
the form
\[ p(X,Y) \leftarrow q_1(X,Z_1),q_2(Z_2,Z_3),\ldots,q_{k-1}(Z_{k-1},Y)
\]
with mode $p(i,o)$.  More conveniently, we could let $Z_0=X$ and
$Z_k=Y$, so the $j$-th literal in the body is $q_j(Z_{j-1},Z_j)$.

We will build up functions using two tools.  The first is a vocabulary
of simple \trm{operations}, each of which performs a small computation
that depend on a few previously-bound vector variables and bind a
single new vector variable. In the implementation, these vectors are
bound in an ``Envir'' (environment) object, which wraps a Python
dictionary.  The second tool is some simple machinery to define a
function by (a) binding some initial variables in an environment and
(b) returning a bound variable after a sequence of operations.  Using
these the function $f(a)$ for a chain can be defined as follows:

\begin{itemize}
\item Bind $\vek{v}_{Z_0} = \vek{u}_a$
\item For $j=1,\ldots,k$, bind $\vek{v}_{Z_j} = \vek{v}_{Z_{j-1}} \M_{q_j}$
\item Return $\vek{v}_{Z_{k}}$
\end{itemize}

\yy{proof}

\subsection{Case 2: A single chain with any variable as output}

Now assume a slightly more complex case, in which the output variable
is some intermediate computation of the chain.
\[ p(Z_0,Z_j) \leftarrow q_1(Z_0,Z_1),q_2(Z_2,Z_3),\ldots,q_{k-1}(Z_{k-1},Z_k)
\]
For instance, we might have
\[ p(X,Y) \leftarrow q(X,Y),s(Y,Z)
\]
which should succeed on $X=a$ for only those $b$'s for which $q(a,b)$
is true, and $\exists c: s(b,c)$.  To achieve this we need a
``backwards'' propagation, where we compute, for each point $j$ in the
chain, the set of constants that could successfully be ``pushed''
through the chain toward the end.  

Hence we will define a set of column vectors $\vek{b}_{Z_j}$ as
follows: $\vek{b}_{Z_k}$ is an all-ones column vector, which we
will denote $\vek{1}^T$, and for all
$j<k$, 
\[ \vek{b}_{Z_j} = \M_{q_j} \vek{b}_{Z_j}
\]
The computation for $f_{\cal M}$ becomes the following.
\begin{itemize}
\item Bind $\vek{v}_{Z_0} = \vek{u}_a$
\item Bind $\vek{b}_{Z_k} = \vek{1}^T$ 
\item For $j=1,\ldots,k$, bind $\vek{v}_{Z_j} = \vek{v}_{Z_{j-1}} \M_{q_j}$
\item For $j=k-1,\ldots,1$, bind $\vek{b}_{Z_j} = \M_{q_j} \vek{b}_{Z_j}$
\item Return $(\vek{b}_{Z_{k}} \vek{v}_{Z_{k}})^T$, which is the
  component-wise product of the two vectors.
\end{itemize}

\yy{proof}

There is an obvious analogy with the forward-backward algorithm,
which we will return to later.

We note that it would be more efficient to compute $\vek{b}_{Z_{k-1}}$
as the row-wise sum of $\M_{q_k}$: this avoids building the large
dense vector $\vek{b}_{Z_k} = \vek{1}^T$, and we do this in the code.
It is also easy to compute $\vek{b}_{Z_{k-1}}$ for literals
$q(Z_{k-1},c)$: in this case $\vek{b}_{Z_{k-1}} = \M_{q_k}[:,c]$, the
$c$-th column of $\M_{q_k}$. \yy{also unary predicates}

\subsection{Case 3: multiple chains}

\yy{next case - two disconnected chains, with proppr clauses
  p(X,pos):-{F:feature(X,F)} and p(X,Y):-q(X,Y) {r} as examples}

\section{Inference in TensorLog}

\yy{graph construction, BP algorithm}

\section{TensorLog Inference and logical inference}

\yy{proof that inference generalizes proof-counting}

\section{Experimental results}

\subsection{Implementation details}

\yy{theano, etc}

\yy{expt1, expt2, ...}

\section{old tree case - to change}

\begin{table}
\hrule

~\\

A sample clause:
\[ p(X,Z) \leftarrow q(X,Y),r(Y,Z),s(Z,W_1),t(Z,W_2),v(X,V).
\] 

The parents of the literals in the clause are as follows:

\begin{tabular}{ll}
literal & parent\\
$q(X,Y)$ & $p(X,Z)$ \\
$r(Y,Z)$ & $q(X,Y)$ \\
$s(Z,W_1)$ & $r(Y,Z)$ \\
$t(Z,W_2)$ & $s(Z,W_1)$ \\
$v(X,V)$ & $p(X,Z)$
\end{tabular}

\caption{Sample ``tree''-like clause}
\label{tab:tree}
\hrule
\end{table}

Consider a clause $A\leftarrow B_1,\ldots,B_k$.  Define the input
variable for $A$ as the input variable specified by the mode ${\cal
  M}$, and likewise the output variable, while for $B_j$, an input
variable is one appearing in a previous $B_{j'}$, for $j'<j$, and an
output variable is one that appears for the first time in $B_{j}$.

Consider a case in which each $B_j$ has one input and one output
variable.  Define \trm{parent} of $B_j$ to be the $B_{j'}$ whose
output is the input of $B_j$, and consider a clause in which the graph
defined by these parent edges is a tree, where the output variable $Y$
of the lefthand side literal $A$ can appear anywhere in the tree.  An
example appears in Table~\ref{tab:tree}. An example of a non-tree
would be the clause below, where $s(W,Y)$ has two inputs and two
parents.
\[ p(X,Y) \leftarrow q(X,W),r(X,Y),s(W,Y).
\] 

Returning to the example of the table, we see there are two issues.
First, we need to check that $X$ is bound to some input $a$ such that
$v(X,V)$ holds, before completing the ``main chain'' $q(X,Y),r(Y,Z)$
which will lead to the final output $Z$.  This suggests that some
additional backward propagation steps are needed.  Second, we need to
include two sets of ``backward'' constraints on $Z$, as $s(Z,W_1)$ and
$t(Z,W_2)$ must hold.

To address these problems we extend the operation sequence to include
some new features.  Above $\vek{b}_{Z_{j}}$ can be thought of as the
set of variables that are in the ``preimage'' of the function
associated with the conjunction $B_j,\ldots,B_k$, but it is also
associated with the variable $Z_j$.  This is inadaquate since $Z_j$
needs to satisfy the constraints associated with possibly several
downstream literals: in the example, $Z$ must satisfy $s(Z,W_1)$ and
$t(Z,W_2)$.  We thus introduce an auxiliary vector $\vek{a}_j$ to
preimage of the subtree rooted at $B_j$. Let $\children(Z)$ be the set
of body literals $B_j$ than use $Z$ as input. Then for any variable
$Z$, we will re-define $\vek{b}_Z$ as
\[ \vek{b}_Z = \prod_{j' \in \children(Z)} \vek{a}_{j'}
\]
where the product is component-wise on the column vectors $\vek{a}_{j'}$,
which are defined as 
\[ \vek{a}_{j} = \M_{q_j} \vek{b}_{Z_j}
\] 
where $Z_j$ is the output variable for $B_j$.

Returning to the first issue, we also need to incorporate the
``backward'' constraints at each point in the ``main chain.''  To be
precise, define the \trm{main chain} literals as those that are on the
path from the literal generating the output $Y$ of the lefthand side
literal $A$ to the root of the tree (which is $A$).  The algorithm
thus be becomes the following.

\begin{itemize}
\item Bind the input variable $X$ of $A$ to $\vek{u}_a$.
\item For each literal $B_j$ on the main chain, from left to right:
   \yy{stopped here}
\item For $j=1,\ldots,k$, bind $\vek{v}_{Z_j} = \vek{v}_{Z_{j-1}} \M_{q_j}$
\item For $j=k-1,\ldots,1$, bind $\vek{b}_{Z_j} = \M_{q_j} \vek{b}_{Z_j}$
\item Return $(\vek{b}_{Z_{k}} \vek{v}_{Z_{k}})^T$, which is the
  component-wise product of the two vectors.
\end{itemize}


\end{document} 
